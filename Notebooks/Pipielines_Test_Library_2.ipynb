{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines Test Library Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import piplines\n",
    "\n",
    "from binary_nets import binary_nets_wrapper # this is actually pipeline1\n",
    "from pipeline2 import pipeline2\n",
    "from pipeline3 import pipeline3\n",
    "from pipeline4 import pipeline4\n",
    "\n",
    "from ensemble_builder import ensemble_builder\n",
    "\n",
    "import image_utils as iu\n",
    "\n",
    "#Import datasets\n",
    "import dataset_generator as dataset\n",
    "\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for all characters\n",
      "Loaded dataset for segmentator\n",
      "Loaded dataset for character a\n",
      "Loaded dataset for character c\n",
      "Loaded dataset for character d_mediana\n",
      "Loaded dataset for character e\n",
      "Loaded dataset for character i\n",
      "Loaded dataset for character m\n",
      "Loaded dataset for character n\n",
      "Loaded dataset for character o\n",
      "Loaded dataset for character r\n",
      "Loaded dataset for character s_mediana\n",
      "Loaded dataset for character t\n",
      "Loaded dataset for character u\n",
      "Loaded dataset for character d_alta\n",
      "Loaded dataset for character s_alta\n",
      "Loaded dataset for character b\n",
      "Loaded dataset for character f\n",
      "Loaded dataset for character h\n",
      "Loaded dataset for character l\n",
      "Loaded dataset for character g\n",
      "Loaded dataset for character p\n",
      "Loaded dataset for character q\n",
      "Loaded dataset for character s_bassa\n"
     ]
    }
   ],
   "source": [
    "(X_train_ocr, y_train_ocr, X_test_ocr, y_test_ocr, _) = dataset.generate_all_chars_with_class(verbose=0, plot=False)\n",
    "print(\"Loaded dataset for all characters\")\n",
    "\n",
    "(X_train_cut, y_train_cut, X_test_cut, y_test_cut) = dataset.generate_dataset_for_segmentator(verbose=0, plot=False)\n",
    "print(\"Loaded dataset for segmentator\")\n",
    "\n",
    "X_train_char = {}\n",
    "y_train_char = {}\n",
    "X_test_char = {}\n",
    "y_test_char = {}\n",
    "\n",
    "for char in dataset.ALPHABET_ALL:\n",
    "    (X_train_char[char], y_train_char[char], X_test_char[char], y_test_char[char]) = \\\n",
    "                                                    dataset.generate_positive_and_negative_labeled(char, verbose=0)\n",
    "    print(\"Loaded dataset for character \" + char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and training of the nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath_b_nets = \"checkpoints/testpip/pipeline1\"\\n\\n# binary nets\\nbinary_nets = {}\\n\\nfor letter in dataset.ALPHABET_ALL:\\n    # Create the binary net\\n    letter_path = os.path.join(path_b_nets, letter)\\n    binary_nets[letter]= ensemble_builder(2, 2, number_of_nets=2,                 path=letter_path, nb_filters1=20, nb_filters2=40, dense_layer_size1=150)\\n    \\n    # Training\\n    binary_nets[letter].fit(X_train_char[letter], y_train_char[letter], X_test_char[letter], y_test_char[letter], forceRetrain=False)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "path_b_nets = \"checkpoints/testpip/pipeline1\"\n",
    "\n",
    "# binary nets\n",
    "binary_nets = {}\n",
    "\n",
    "for letter in dataset.ALPHABET_ALL:\n",
    "    # Create the binary net\n",
    "    letter_path = os.path.join(path_b_nets, letter)\n",
    "    binary_nets[letter]= ensemble_builder(2, 2, number_of_nets=2, \\\n",
    "                path=letter_path, nb_filters1=20, nb_filters2=40, dense_layer_size1=150)\n",
    "    \n",
    "    # Training\n",
    "    binary_nets[letter].fit(X_train_char[letter], y_train_char[letter], X_test_char[letter], y_test_char[letter], forceRetrain=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_cut_classifier = \"checkpoints/letter_not_letter\"\n",
    "\n",
    "cut_class = ensemble_builder(2, 800, number_of_nets=2,\\\n",
    "                 path=path_cut_classifier, nb_filters1=50, nb_filters2=100,\\\n",
    "                 dense_layer_size1=250)\n",
    "\n",
    "#cut_class.fit(X_train_cut, y_train_cut, X_test_cut, y_test_cut, forceRetrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocr classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_ocr_class = \"checkpoints/09_22-classes\"\n",
    "\n",
    "ocr_classifier = ensemble_builder(22, 800, number_of_nets=2, path=path_ocr_class,\\\n",
    "            nb_filters1=50, nb_filters2=100, dense_layer_size1=250)\n",
    "\n",
    "#ocr_classifier.fit(X_train_ocr, y_train_ocr, X_test_ocr, y_test_ocr, forceRetrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip1 = binary_nets_wrapper(binary_nets)\\n\\nprediction_pip1 = pip1.predict(X_test_ocr)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip1 = binary_nets_wrapper(binary_nets)\n",
    "\n",
    "prediction_pip1 = pip1.predict(X_test_ocr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pip2 = pipeline2(cut_class, ocr_classifier)\n",
    "\n",
    "asseras_good_cuts = iu.open_many_samples( \\\n",
    "                    [\"not_code/words/good_cuts/asseras/a1.png\",\n",
    "                     \"not_code/words/good_cuts/asseras/f1.png\",\n",
    "                     \"not_code/words/good_cuts/asseras/f2.png\",\n",
    "                     \"not_code/words/good_cuts/asseras/e.png\",\n",
    "                     \"not_code/words/good_cuts/asseras/r.png\",\n",
    "                     \"not_code/words/good_cuts/asseras/a2.png\",\n",
    "                     \"not_code/words/good_cuts/asseras/s.png\"])\n",
    "\n",
    "prediction_pip2 = pip2.predict(asseras_good_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, array([('a',   9.99758005e+01), ('o',   1.13711780e-02),\n",
       "         ('i',   7.22903933e-03)], \n",
       "        dtype=[('letters', '<U16'), ('grades', '<f8')])),\n",
       " (True,\n",
       "  array([('s_alta',  96.79138064), ('l',   3.00127901), ('f',   0.20640711)], \n",
       "        dtype=[('letters', '<U16'), ('grades', '<f8')])),\n",
       " (False, []),\n",
       " (True, array([('e',  98.46054316), ('c',   1.17276963), ('o',   0.17953472)], \n",
       "        dtype=[('letters', '<U16'), ('grades', '<f8')])),\n",
       " (True, array([('r',  95.26131153), ('e',   3.89830396), ('i',   0.29342179)], \n",
       "        dtype=[('letters', '<U16'), ('grades', '<f8')])),\n",
       " (True, array([('a',   9.99343634e+01), ('o',   2.17619818e-02),\n",
       "         ('i',   2.11800376e-02)], \n",
       "        dtype=[('letters', '<U16'), ('grades', '<f8')])),\n",
       " (True, array([('s_alta',   9.99579728e+01), ('f',   4.13175847e-02),\n",
       "         ('l',   7.13807640e-04)], \n",
       "        dtype=[('letters', '<U16'), ('grades', '<f8')]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pip2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip3 = pipeline3(pip1, ocr_classifier)\\n\\nprediction_pip3 = pip3.predict(X_test_ocr)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip3 = pipeline3(pip1, ocr_classifier)\n",
    "\n",
    "prediction_pip3 = pip3.predict(X_test_ocr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pipeline 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip4 = pipeline4(cut_class, pip1)\\n\\nprediction_pip4 = pip4.predict(X_test_ocr)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip4 = pipeline4(cut_class, pip1)\n",
    "\n",
    "prediction_pip4 = pip4.predict(X_test_ocr)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
